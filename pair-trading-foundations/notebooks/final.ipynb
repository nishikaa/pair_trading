{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c2d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import ta\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#machine learning/statistical dependencies\n",
    "import statsmodels.api as sm\n",
    "from kneed import KneeLocator\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, silhouette_score\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation\n",
    "from sklearn.manifold import TSNE\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.stattools as ts\n",
    "from statsmodels.tsa.arima.model import ARIMA \n",
    "# from pykalman import KalmanFilter\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "import scipy.cluster.hierarchy as shc\n",
    "import plotly.express as px\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75539ad-56e2-416e-9166-e470691bb23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pair_trading_foundations.utils import GetSP500Data\n",
    "import warnings\n",
    "# The yfinance module has some deprecation warning\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e159bf3-2d84-4011-a7cf-df3b66042f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "earliest_date = '2008-12-01'\n",
    "todays_date = datetime.today().strftime('%Y-%m-%d')\n",
    "session = requests.Session()\n",
    "session.verify = False\n",
    "print(f'Extracting date range: [ {earliest_date} ]-[ {todays_date} ]')\n",
    "all_data = GetSP500Data(earliest_date,todays_date).get_all_sp_tickers().get_consolidated_data();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe557d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = all_data.copy()\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75283eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Null Values in final_data. Examine Date, Close and daily_returns.\n",
    "final_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da29728",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_data.pivot(index='Date', columns='Ticker', values='Close') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff37e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data = df.copy()\n",
    "\n",
    "# Null Values are Examined for Each Individual Stock on a Basis of Descriptive Statistics.\n",
    "\n",
    "print('Null Values =',exp_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d31ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    " print('Data Shape Before Cleaning =', exp_data.shape)\n",
    "\n",
    "missing_percentage = exp_data.isnull().mean().sort_values(ascending=False)\n",
    "dropped_list = sorted(list(missing_percentage[missing_percentage > 0].index))\n",
    "print(f\"The following is a list of all stocks with ticker-level NA values: \\n{dropped_list}\")\n",
    "exp_data.drop(labels=dropped_list, axis=1, inplace=True)\n",
    "\n",
    "print('Data Shape After Cleaning =', exp_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data = exp_data.fillna(exp_data.median()) ### This step to ensure we did not miss any odd days, and if we did, it is standard industry practice to take the median\n",
    "\n",
    "exp_data['year']=exp_data.index.year\n",
    "\n",
    "exp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare clustering data by splitting train and test to match trading strategy later\n",
    "\n",
    "cluster_train_size = int(len(exp_data) * 0.7)\n",
    "\n",
    "# Split the data into training, validation and test sets based on these lengths:\n",
    "cluster_train = exp_data[0:cluster_train_size]\n",
    "print(f\"The length of the training set is: {len(cluster_train)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0e0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(cluster_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a8097",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4635a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average returns by year by stock\n",
    "returns_year = cluster_train.groupby('year').mean()\n",
    "\n",
    "# calculate return percent change, then aggregated returns and volatility by stock\n",
    "returns = returns_year.pct_change().mean()\n",
    "\n",
    "returns = pd.DataFrame(returns)\n",
    "returns.columns = ['returns']\n",
    "returns['volatility'] = returns_year.pct_change().dropna().std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b306ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_data = returns.copy()\n",
    "ret_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f8dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize The Clustering Dataset Along a t-Distributed Curve\n",
    "scale = StandardScaler().fit(ret_data)\n",
    "\n",
    "# Transform the Original Clustering Dataset to These Standardized Values\n",
    "scaled_data = pd.DataFrame(scale.fit_transform(ret_data),columns = ret_data.columns, index = ret_data.index) \n",
    "X = scaled_data\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41426dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 15\n",
    "random_state = 15\n",
    "\n",
    "# Algorithm:\n",
    "# 1. Randomly pick k centroids from the examples ploted above as initial cluster centers (ğœ‡ğ‘—,ğ‘—âˆˆ1,...,ğ‘˜)\n",
    "# 2. Assign each example to the nearest centroid,  ğœ‡ğ‘—\n",
    "# 3. Move the centroids to the center of the examples that were assigned to it\n",
    "# 4. Repeat (2-3) until the cluster assignments do not change or a maximum number of iterations is reached.\n",
    "\n",
    "def fit_kmeans(X, n_clusters, random_state):\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters,\n",
    "                    init='k-means++',\n",
    "                    n_init=15,\n",
    "                    max_iter=300,\n",
    "                    tol=1e-04)\n",
    "    model = kmeans.fit(X)\n",
    "    return model\n",
    "\n",
    "model = fit_kmeans(X, n_clusters, random_state)\n",
    "print(model)\n",
    "\n",
    "## get cluster prediction and add to DataFrame\n",
    "kX = X.copy()\n",
    "kX['cluster'] = model.predict(X)\n",
    "kX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a388d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = clustered_series.value_counts()\n",
    "\n",
    "cluster_vis_list = list(counts[(counts<20) & (counts>1)].index)[::-1]\n",
    "\n",
    "clust1 = pd.DataFrame(clustered_series,columns=['label'])\n",
    "grouped_df = clust1.groupby('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aa3a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_train = final_data.copy()\n",
    "final_data_train = final_data_train.loc[final_data_train.Date <= max(cluster_train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c9507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_data = final_data_train.pivot(index='Ticker', columns='Date', values='Adj Close')\n",
    "pivot_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c9a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster evaluation and model selection\n",
    "cluster_size_limit = 9999\n",
    "counts = clustered_series.value_counts()\n",
    "ticker_count = counts[(counts>1) & (counts<=cluster_size_limit)]\n",
    "print (\"Number of clusters: %d\" % len(ticker_count))\n",
    "print (\"Number of Pairs: %d\" % (ticker_count*(ticker_count-1)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91211dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide a functional definition for the cointegrated augmented dickey fuller test\n",
    "def find_cadf_pairs(data):\n",
    "    n = data.shape[1]\n",
    "    score_matrix = np.zeros((n, n))\n",
    "    pvalue_matrix = np.ones((n, n))\n",
    "    keys = data.keys()\n",
    "    pairs = {}\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            S1 = data[keys[i]]\n",
    "            S2 = data[keys[j]]\n",
    "            ols = sm.OLS(S1,S2).fit()\n",
    "            pred = ols.predict(S2)\n",
    "            adf = ts.adfuller(S1-pred)\n",
    "            score = adf[0]\n",
    "            pvalue = adf[1]\n",
    "            score_matrix[i, j] = score\n",
    "            pvalue_matrix[i, j] = pvalue\n",
    "            if pvalue < 0.03:\n",
    "                pairs[(keys[i], keys[j])] = adf\n",
    "    return score_matrix, pvalue_matrix, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeebab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Cointegrated Pairs. This takes approximately 30-40 minutes. Again, this would take much longer if clustering analysis was not applied.\n",
    "adf_dict = {}\n",
    "\n",
    "for i, clust in enumerate(ticker_count.index):\n",
    "    tickers = clustered_series[clustered_series == clust].index\n",
    "    score_matrix, pvalue_matrix, adf_pairs = find_cadf_pairs(cluster_train[tickers])\n",
    "    adf_dict[clust] = {}\n",
    "    adf_dict[clust]['score_matrix'] = score_matrix\n",
    "    adf_dict[clust]['pvalue_matrix'] = pvalue_matrix\n",
    "    adf_dict[clust]['pairs'] = adf_pairs\n",
    "    \n",
    "adf_pairs = []   \n",
    "for cluster in adf_dict.keys():\n",
    "    adf_pairs.extend(adf_dict[cluster]['pairs'])\n",
    "    \n",
    "print (\"Number of pairs:\", len(adf_pairs))\n",
    "print (\"In those pairs, we found %d unique tickers.\" % len(np.unique(adf_pairs)))\n",
    "print(adf_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top500_pairs = list(map(list, adf_pairs))\n",
    "top500_pairs = [lst for lst in top500_pairs if lst and not any(x is None for x in lst)]\n",
    "top500_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39305f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification of the ADF test from earlier to apply to the top pairs in each cluster. Pulling out the p-value only.\n",
    "def find_cadf_pairs_validate(data):\n",
    "    n = data.shape[1]\n",
    "    score_matrix = np.zeros((n, n))\n",
    "    pvalue_matrix = np.ones((n, n))\n",
    "    keys = data.keys()\n",
    "    pairs = {}\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            S1 = data[keys[i]]\n",
    "            S2 = data[keys[j]]\n",
    "            ols = sm.OLS(S1,S2).fit()\n",
    "            pred = ols.predict(S2)\n",
    "            adf = ts.adfuller(S1-pred)\n",
    "            score = adf[0]\n",
    "            pvalue = adf[1]\n",
    "            score_matrix[i, j] = score\n",
    "            pvalue_matrix[i, j] = pvalue\n",
    "            if pvalue < 0.05:\n",
    "                pairs[(keys[i], keys[j])] = adf\n",
    "    pvalue_result = min(pvalue_matrix[0])\n",
    "    return pvalue_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop the best cluster list to filter exp_data df\n",
    "# loop the filtered df through find_cadf_pairs_validate function to retrieve the pvalue_result\n",
    "# append the pvalue_result to a new dataframe to rank the best pair based on p-value\n",
    "\n",
    "def cluster_ranking(data):\n",
    "    n = len(data)\n",
    "    cluster_ranked = pd.DataFrame({'ticker1':[],'ticker2':[],'pvalue':[]})\n",
    "    for i in range(n):\n",
    "        pair_validate_df=exp_data.filter(items=data[i])\n",
    "        p_value = find_cadf_pairs_validate(pair_validate_df)\n",
    "        new_row = {'ticker1':top500_pairs[i][0],'ticker2':top500_pairs[i][1],'pvalue': p_value}\n",
    "        cluster_ranked = cluster_ranked.append(new_row,ignore_index=True)\n",
    "        new_row = {}\n",
    "    cluster_ranked = cluster_ranked.sort_values(by='pvalue')\n",
    "    return cluster_ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788bf3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the best pair from each cluster, sorted by the lowest p-value\n",
    "top500_df = cluster_ranking(top500_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eadaf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top500_df = top500_df[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b26afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top500_df = top500_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a7ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "top500_df.to_csv('top500pairs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
